

import sys
import os
import sqlite3
import pandas as pd
import yaml
from tqdm import tqdm
from datetime import datetime
import scrape_ufc_stats_library as LIB
# -*- coding: utf-8 -*-
"""Modules.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BOB0qo3SP-TVq7yji7X7vUUG2QYdHD26
"""

class Scrapper():
  def __init__(self, config, conn):
    self.config = config
    self.conn = conn
    pass
  def process(self):
    pass
  def save(self):
    pass

class Eventdetails(Scrapper):
  def __init__(self, config, conn):
    super().__init__(config, conn)
  def process(self, target_date=None):
    events_url = self.config['completed_events_all_url']
    # get soup
    soup = LIB.get_soup(events_url)

    # parse event details
    all_event_details_df = LIB.parse_event_details(soup)

    # convert date to datetime
    all_event_details_df['DATE'] = pd.to_datetime(all_event_details_df.DATE)
    all_event_details_df['DATE'] = all_event_details_df['DATE'].dt.strftime('%Y-%m-%d')
    if(target_date != None):
        all_event_details_df  = all_event_details_df[all_event_details_df['DATE'] > target_date]
    self.all_event_details_df = all_event_details_df
    return all_event_details_df
  def save(self):
    self.all_event_details_df.to_sql(os.path.splitext(self.config['event_details_file_name'])[0], self.conn, if_exists='replace', index=False)
    pass

import scrape_ufc_stats_library as LIB
from tqdm.notebook import tqdm_notebook

class Fightdetails(Scrapper):
  def __init__(self, config, conn, event_urls):
    self.event_urls = event_urls
    super().__init__(config,conn)
  def process(self):
    list_of_events_urls = list(self.event_urls)
    # create empty df to store fight details
    all_fight_details_df = pd.DataFrame(columns=self.config['fight_details_column_names'])
    # loop through each event and parse fight details
    for url in tqdm(list_of_events_urls):
        # get soup
        soup = LIB.get_soup(url)
        # parse fight links
        fight_details_df = LIB.parse_fight_details(soup)
        # concat fight details
        all_fight_details_df = pd.concat([all_fight_details_df, fight_details_df])
    self.all_fight_details_df = all_fight_details_df
    return all_fight_details_df
  def save(self):
    self.all_fight_details_df.to_sql(os.path.splitext(self.config['fight_details_file_name'])[0], self.conn, if_exists='replace', index=False)

import scrape_ufc_stats_library as LIB
from tqdm.notebook import tqdm_notebook

class Fightresultsandstats(Scrapper):
  def __init__(self, config, conn, fight_details_urls):
    self.fight_details_urls = fight_details_urls
    super().__init__(config, conn)
  def process(self):
    # create empty df to store fight results
    all_fight_results_df = pd.DataFrame(columns=self.config['fight_results_column_names'])
    # create empty df to store fight stats
    all_fight_stats_df = pd.DataFrame(columns=self.config['fight_stats_column_names'])

    # loop through each fight and parse fight results and stats
    for url in tqdm(self.fight_details_urls):

        # get soup
        soup = LIB.get_soup(url)

        try:
          # parse fight results and fight stats
          fight_results_df, fight_stats_df = LIB.parse_organise_fight_results_and_stats(
              soup,
              url,
              self.config['fight_results_column_names'],
              self.config['totals_column_names'],
              self.config['significant_strikes_column_names']
              )
        except:
          print(f'Error parsing {url}')
          print(soup)
          print(fight_results_df)
          print(fight_stats_df)
          continue

        # concat fight results
        all_fight_results_df = pd.concat([all_fight_results_df, fight_results_df])
        # concat fight stats
        all_fight_stats_df = pd.concat([all_fight_stats_df, fight_stats_df])
    self.all_fight_results_df = all_fight_results_df
    self.all_fight_stats_df = all_fight_stats_df
    return all_fight_results_df, all_fight_stats_df

  def save(self):
      self.all_fight_results_df.to_sql(os.path.splitext(self.config['fight_results_file_name'])[0], self.conn, if_exists='replace', index=False)

      # write to fileqasdewwed
      #all_fight_stats_df.to_csv(config['fight_stats_file_name'], index=False)
      
      # write to sqlite
      self.all_fight_stats_df.to_sql(os.path.splitext(self.config['fight_stats_file_name'])[0], self.conn, if_exists='replace', index=False)

import scrape_ufc_stats_library as LIB
from tqdm.notebook import tqdm_notebook

class Fighterdetails(Scrapper):
  def __init__(self, config, conn):
    super().__init__(config, conn)
  def process(self, filtered_data = None):
    # generate list of urls for fighter details
    list_of_alphabetical_urls = LIB.generate_alphabetical_urls()
    print("completed generating alphabetical urls", list_of_alphabetical_urls)
    
    
    #if (filtered_data != None):
        

    # create empty dataframe to store all fighter details
    all_fighter_details_df = pd.DataFrame()

    # loop through list of alphabetical urls
    for url in tqdm(list_of_alphabetical_urls):
        # get soup
        soup = LIB.get_soup(url)
        # parse fighter details
        fighter_details_df = LIB.parse_fighter_details(soup, self.config['fighter_details_column_names'])
        # concat fighter_details_df to all_fighter_details_df
        all_fighter_details_df = pd.concat([all_fighter_details_df, fighter_details_df])
    
    #filter all_fighter_details_df to only contain rows with fighters present in filtered_data
    #add a 'FULL_NAME' column to all_fighter_details_df
  # Ensure 'FIRST' and 'LAST' exist and replace NaN with empty strings
    # Ensure 'FIRST' and 'LAST' columns exist and handle missing values
# Ensure 'FIRST' and 'LAST' exist and replace NaN with empty strings
    if 'FIRST' in all_fighter_details_df.columns and 'LAST' in all_fighter_details_df.columns:
        all_fighter_details_df['FIRST'] = all_fighter_details_df['FIRST'].fillna('')
        all_fighter_details_df['LAST'] = all_fighter_details_df['LAST'].fillna('')
        
        # Construct FULL_NAME correctly, removing any leading/trailing spaces
        all_fighter_details_df['FULL_NAME'] = (all_fighter_details_df['FIRST'] + ' ' + all_fighter_details_df['LAST']).str.strip()
    else:
        print("Error: 'FIRST' or 'LAST' column is missing from all_fighter_details_df.")
        print("Available columns:", all_fighter_details_df.columns)
        return None

    # Ensure 'FIGHTER' column exists in filtered_data before filtering
    if 'FIGHTER' not in filtered_data.columns:
        print("Error: 'FIGHTER' column is missing in filtered_data.")
        print("Available columns in filtered_data:", filtered_data.columns)
        return None

    # Remove NaNs in FIGHTER column (if any) and standardize format
    filtered_data['FIGHTER'] = filtered_data['FIGHTER'].fillna('').str.strip()

    # Debug: Print unique names before filtering to check for mismatches
    print("Unique FULL_NAMEs in all_fighter_details_df:", all_fighter_details_df['FULL_NAME'].unique()[:5])
    print("Unique FIGHTER names in filtered_data:", filtered_data['FIGHTER'].unique()[:5])

    # Filter based on matching FULL_NAMEs
    all_fighter_details_df = all_fighter_details_df[all_fighter_details_df['FULL_NAME'].isin(filtered_data['FIGHTER'])]

    # Drop 'FULL_NAME' column if not needed
    all_fighter_details_df = all_fighter_details_df.drop(columns=['FULL_NAME'])

        
    self.all_fighter_details_df = all_fighter_details_df

    return all_fighter_details_df
  def save(self):
    self.all_fighter_details_df.to_sql(os.path.splitext(self.config['fighter_details_file_name'])[0], self.conn, if_exists='replace', index=False)

class Fightertott(Scrapper):
  def __init__(self, config, conn, list_of_fighter_urls):
    self.list_of_fighter_urls = list_of_fighter_urls
    super().__init__(config, conn)
  def process(self):
    # create empty df to store fighters' tale of the tape
    all_fighter_tott_df = pd.DataFrame(columns=self.config['fighter_tott_column_names'])

    #create a shorter version of list_of_fighter_urls
    #list_of_fighter_urls_100 = list_of_fighter_urls[0:100]
    
    # loop through list_of_fighter_urls
    for url in tqdm(self.list_of_fighter_urls):
        # get soup
        soup = LIB.get_soup(url)
        # parse fighter tale of the tape
        fighter_tott = LIB.parse_fighter_tott(soup)
        # organise fighter tale of the tape
        fighter_tott_df = LIB.organise_fighter_tott(fighter_tott, self.config['fighter_tott_column_names'], url)
        # concat fighter
        all_fighter_tott_df = pd.concat([all_fighter_tott_df, fighter_tott_df])

      # Convert the DOB column to datetime, coercing invalid entries to NaT
    all_fighter_tott_df['DOB'] = pd.to_datetime(all_fighter_tott_df['DOB'], errors='coerce')

    # Format the valid dates to 'YYYY-MM-DD'
    all_fighter_tott_df['DOB'] = all_fighter_tott_df['DOB'].dt.strftime('%Y-%m-%d')
    
    self.all_fighter_tott_df = all_fighter_tott_df

    return all_fighter_tott_df
  def save(self):
    self.all_fighter_tott_df.to_sql(os.path.splitext(self.config['fighter_tott_file_name'])[0], self.conn, if_exists='replace', index=False)